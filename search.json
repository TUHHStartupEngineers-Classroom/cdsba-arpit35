[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Load the data variable:\n\n\nrandom_vars &lt;- readRDS(\"Causal_Data_Science_Data/random_vars.rds\")\nrandom_vars\n\n\n\n  \n\n\n\n\nTask 1\n\nFor each variable: age and income, computing the following values:\n\nExpected Value\n\n\nAge\n\n\nexpected_value_age &lt;- mean(random_vars$age)\npaste(\"Expected Value =\", expected_value_age)\n\n#&gt; [1] \"Expected Value = 33.471\"\n\n\n\nIncome\n\n\nexpected_value_income &lt;- mean(random_vars$income)\npaste(\"Expected Value =\", expected_value_income)\n\n#&gt; [1] \"Expected Value = 3510.731\"\n\n\n\nVariance\n\n\nAge\n\n\nvariance_age &lt;- var(random_vars$age)\npaste(\"Variance Value =\", variance_age)\n\n#&gt; [1] \"Variance Value = 340.607766766767\"\n\n\n\nIncome\n\n\nvariance_income &lt;- var(random_vars$income)\npaste(\"Variance Value =\", variance_income)\n\n#&gt; [1] \"Variance Value = 8625645.84448348\"\n\n\n\nStandard Deviation\n\n\nAge\n\n\nstandard_deviation_age &lt;- sd(random_vars$age)\npaste(\"Standard Deviation Value =\", standard_deviation_age)\n\n#&gt; [1] \"Standard Deviation Value = 18.4555619466536\"\n\n\n\nIncome\n\n\nstandard_deviation_income &lt;- sd(random_vars$income)\npaste(\"Standard Deviation Value =\", standard_deviation_income)\n\n#&gt; [1] \"Standard Deviation Value = 2936.94498492626\"\n\n\n\n\nTask 2\n\nCompare the standard deviations of age and income directly is not a good idea. This is because age and income are measured in different units, so comparing their standard deviations may not give us useful information. Standard deviations show how spread out the values are within each group (age or income), but they don’t allow for a straightforward comparison between the two groups.\n\n\nTask 3\n\n\nThe connection between two things using covariance and correlation:\n\n\nCovariance\n\n\ncovariance &lt;- cov(random_vars$age, random_vars$income)\npaste(\"Covariance =\", covariance)\n\n#&gt; [1] \"Covariance = 29700.1468458458\"\n\n\n\nCorrelation\n\n\ncorrelation &lt;- cor(random_vars$age, random_vars$income)\npaste(\"Correlation =\", correlation)\n\n#&gt; [1] \"Correlation = 0.547943162326477\"\n\n\n\n\nTask 4\n\n\nCovariance:\n\n\nCovariance indicates the extent to which two factors vary together, and this variation can be either positive, negative, or zero. The measurement scale of covariance is determined by multiplying the units of the two variables being studied. However, a drawback of covariance is its lack of standardization in scale, making it challenging to compare the strength of relationships between different pairs of variables.\n\n\nCorrelation:\n\n\nOn the other hand, correlation provides a standardized way of measuring the relationship between things, assigning a value between -1 and 1. A correlation of 1 indicates a flawless positive connection, -1 signifies a complete negative relationship, and 0 suggests no linear relationship. The standardized scale of correlation simplifies comparison and comprehension across various pairs of things. Due to its standardized scale, correlation is more straightforward to grasp compared to covariance, allowing for easy comparison of the strength and direction of relationships between different pairs of things.\n\n\nIn summary:\n\n\nIn general, correlation is more straightforward to comprehend and compare than covariance. This is because correlation provides a standardized measurement that falls within the range of -1 to 1. This standardized scale simplifies the understanding of the strength and direction of the relationship between two things. If your goal is to evaluate the connection between two variables without encountering difficulties related to scale, correlation is typically a preferable option over covariance.\n\n\n\nTask 5\n\n\nCompute the conditional expected values for the following:\n\n\n\\(E[income|age&lt;=18]\\):\n\n\nexpected_value_teens &lt;- mean(random_vars$income[random_vars$age &lt;= 18])\npaste(\"E[income|age &lt;= 18] =\", expected_value_teens)\n\n#&gt; [1] \"E[income|age &lt;= 18] = 389.607438016529\"\n\n\n\n\\(E[income|age\\in[18,65)]\\)\n\n\nexpected_value_adults &lt;- mean(random_vars$income[random_vars$age &gt;= 18 & random_vars$age &lt; 65])\npaste(\"E[income|age ∈ [18, 65]] =\", expected_value_adults)\n\n#&gt; [1] \"E[income|age ∈ [18, 65]] = 4685.73426573427\"\n\n\n\n\\(E[income|age&gt;=65]\\)\n\n\nexpected_value_seniors &lt;- mean(random_vars$income[random_vars$age &gt;= 65])\npaste(\"E[income|age &gt;= 65] =\", expected_value_seniors)\n\n#&gt; [1] \"E[income|age &gt;= 65] = 1777.23728813559\""
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "Task 1\n\n\nlibrary(tidyverse)\n# number of observations\nn &lt;- 100\n\n# Create tibble\nlm_dat &lt;- tibble(\n  # draw from normal distribution\n  cooking_time = rnorm(n, mean = 4, sd = 1),\n  # food_price depends on cooking_time and noise from normal distribution\n  food_price= 0.3*cooking_time + rnorm(n, 0, 0.1)\n)\n\n\nlm_dat\n\n\n\n  \n\n\n\n\nlibrary(ggplot2)\n\nggplot(lm_dat, aes(x = cooking_time, y = food_price)) +\n  geom_point() +\n  labs(title = \"Cooking Time vs. Food Price\",\n       x = \"The total time taken to cook the food\",\n       y = \"The price of the food\")"
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "Task - 1\n\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#&gt; ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n#&gt; ✔ purrr     1.0.2     ✔ tidyr     1.3.0\n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks ggdag::filter(), stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ✖ dplyr::recode() masks car::recode()\n#&gt; ✖ purrr::some()   masks car::some()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\n\niv_expl &lt;- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ U,\n  D ~ Z,\n  exposure = \"D\",\n  latent = \"U\",\n  outcome = \"Y\",\n  coords = list(x = c(U = 1, D = 0, Y = 2, Z = -1),\n                y = c(U = 1, D = 0, Y = 0, Z = 0)),\n  labels = c(\"D\" = \"Pop-up Feature\", \n             \"Y\" = \"App Usage\", \n             \"U\" = \"Unobserved Factor\",\n             \"Z\" = \"Encouragement\")\n)\nggdag(iv_expl, text = T) +\n  guides(color = \"none\") +\n  theme_minimal() + \n  geom_dag_point() +\n  geom_dag_text() +\n  geom_dag_edges() +\n  geom_dag_label_repel(aes(label = label))\n\n\n\n\n\n\n\n\n\n\nTask - 2\n\n\ndf &lt;- readRDS(\"Causal_Data_Science_Data/rand_enc.rds\")\ndf\n\n\n\n  \n\n\n\n\nmodel_biased &lt;- lm(time_spent ~ used_ftr, data = df)\nsummary(model_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nTask - 3\n\n\ncor(df) %&gt;% round(2)\n\n#&gt;            rand_enc used_ftr time_spent\n#&gt; rand_enc       1.00     0.20       0.13\n#&gt; used_ftr       0.20     1.00       0.71\n#&gt; time_spent     0.13     0.71       1.00\n\n\n\nLooking at your correlation matrix, I observe the following correlations:\n\n\nThe correlation between the randomly selected customer (rand_enc) and features used (used_ftr) is 0.20, suggesting a weak positive relationship. The correlation between rand_enc and time_spent is 0.13, indicating a weak positive relationship. The correlation between used_ftr and time_spent is 0.71, suggesting a moderate positive relationship.\n\n\nArguments for Instrumental Variable Estimation:\n\n\nCorrelation Weakness: The correlations between the randomly selected customer and the other variables are relatively weak, indicating potential endogeneity issues. In such cases, instrumental variable estimation can help mitigate bias.\nOmitted Variable Bias: If there are unobserved factors affecting both the features used and time spent, instrumental variable estimation might be useful in addressing omitted variable bias.\n\n\nConsiderations Against Instrumental Variable Estimation:\n\n\nValidity of the Instrument: For instrumental variable estimation to be valid, the chosen instrument must be relevant and not directly correlated with the outcome variable (time_spent) except through its correlation with the features used.\nEndogeneity Severity: The strength of the endogeneity issue is crucial. If the endogeneity is not substantial, other techniques like controlling for covariates may be sufficient.\n\n\n\nTask - 4\n\n\nlibrary(estimatr)\n\nmodel_iv &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = df)\nsummary(model_iv)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = df)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe initial estimate is 10.82, but the Instrumental Variable (IV) estimate is slightly lower at 9.73. The initial estimate, also called the naive estimate, tends to be higher because it doesn’t consider hidden factors linked to both feature usage and time spent on the app (referred to as encouragement). The IV estimate addresses this by considering these hidden factors, leading to a more accurate, slightly lower estimate. The naive estimate not only reflects the impact of feature usage on app time but also includes the influence of encouragement, causing it to be too high."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "df  &lt;- readRDS(\"Causal_Data_Science_Data/membership.rds\")\ndf\n\n\n\n  \n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\n\n# Define the DAG\ndag &lt;- 'dag {\n  card [pos=\"0.1,0.5\"]\n  avg_purch [pos=\"0.5,0.5\"]\n  pre_avg_purch [pos=\"0.3,1\"]\n  pre_avg_purch -&gt; card\n  pre_avg_purch -&gt; avg_purch\n  card -&gt; avg_purch\n}'\n\n# Convert the text to a dagitty object\ndag &lt;- dagitty(dag)\n\n# Plot the DAG using ggdag\nggdag(dag) + theme_dag() + geom_dag_label_repel(aes(label = name))"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\ndag &lt;- 'dag {\nbb=\"0,0,1,1\"\nParking [exposure,pos=\"0.1,0.5\"]\nSales [outcome,pos=\"0.5,0.5\"]\nLocation [pos=\"0.3,1\"]\nParking -&gt; Sales\nLocation -&gt; Parking\nLocation -&gt; Sales\n}\n'\nggdag_status(dag) + theme_dag()"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "Loading the data frame\n\n\ndf  &lt;- readRDS(\"Causal_Data_Science_Data/abtest_online.rds\")\ndf\n\n\n\n  \n\n\n\n\nChecking whether the covariates are balanced across the groups\n\nlibrary(ggplot2)\n\ncompare_purchase_amount &lt;- \n  ggplot(df, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"purchase_amount\", title = \"Difference in purchase amount\")\n  compare_purchase_amount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe two plots above show whether the different characteristics are balanced between the two groups.\n\n\n\n\nRunning a regression to find the effect of chatbot on sales.\n\nlm_sales_ate &lt;- lm(purchase_amount ~ chatbot, data = df)\nsummary(lm_sales_ate)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe coefficient of the chatbot is -7.0756, which means the presence of the chatbot has a negative influence on sales.\n\n\n\n\nFinding subgroup-specific effects by including an interaction\n\nlm_cate4 &lt;- lm(purchase_amount ~ chatbot + I(chatbot^2) * mobile_device + chatbot * previous_visit, data = df)\nsummary(lm_cate4)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot + I(chatbot^2) * mobile_device + \n#&gt;     chatbot * previous_visit, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -22.057 -15.928  -7.419  12.804  65.333 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                                Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                     17.2052     1.2982  13.253  &lt; 2e-16 ***\n#&gt; chatbotTRUE                    -11.1064     1.8434  -6.025 2.38e-09 ***\n#&gt; I(chatbot^2)                         NA         NA      NA       NA    \n#&gt; mobile_deviceTRUE               -0.8791     1.7823  -0.493 0.621943    \n#&gt; previous_visit                  -0.1030     0.3749  -0.275 0.783562    \n#&gt; I(chatbot^2):mobile_deviceTRUE   0.2044     2.5148   0.081 0.935229    \n#&gt; chatbotTRUE:previous_visit       2.0978     0.5781   3.629 0.000299 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.49 on 994 degrees of freedom\n#&gt; Multiple R-squared:  0.05495,    Adjusted R-squared:  0.0502 \n#&gt; F-statistic: 11.56 on 5 and 994 DF,  p-value: 7.177e-11\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above result gives the best R-squared value from the interaction I had.\n\n\n\n\nUsing the outcome variable purchase and runing a logistic regression.\n\nlm_model_binary &lt;- glm(purchase ~ chatbot * mobile_device + chatbot * previous_visit, family = binomial(link = 'logit'), data = df)\nsummary(lm_model_binary)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot * mobile_device + chatbot * \n#&gt;     previous_visit, family = binomial(link = \"logit\"), data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)                    0.03872    0.14047   0.276    0.783    \n#&gt; chatbotTRUE                   -1.65686    0.22511  -7.360 1.84e-13 ***\n#&gt; mobile_deviceTRUE             -0.05177    0.19284  -0.268    0.788    \n#&gt; previous_visit                -0.01769    0.04061  -0.436    0.663    \n#&gt; chatbotTRUE:mobile_deviceTRUE -0.01616    0.29818  -0.054    0.957    \n#&gt; chatbotTRUE:previous_visit     0.32646    0.06912   4.723 2.32e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1238.6  on 994  degrees of freedom\n#&gt; AIC: 1250.6\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nlogistic regression is tailored for binary or categorical outcomes, and its interpretation revolves around odds ratios and probabilities."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "Loading the data frame\nlibrary(dplyr)\n\ndf_hospdd  &lt;- readRDS(\"Causal_Data_Science_Data/hospdd.rds\")\ndf_hospdd"
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "Assignment 1\n\n\nGiven Probabilities - \\(P(S) = 0.3\\), \\(P(\\overline{S}) = 0.7\\), \\(P(T/S) = 0.2\\), \\(P(\\overline{T}/S) = 0.8\\), \\(P(T/\\overline{S}) = 0.6\\), \\(P(\\overline{T}/\\overline{S}) = 0.4\\)\n\n\n\\(P(T\\cap S) = P(S) * P(T/S)\\)\n\n\nP_T_and_S &lt;- 0.3 * 0.2\nP_T_and_S\n\n#&gt; [1] 0.06\n\n\n\n\\(P(T\\cap \\overline{S}) = P(\\overline{S}) * P(T/\\overline{S})\\)\n\n\nP_T_and_S_not &lt;- 0.7 * 0.6\nP_T_and_S_not\n\n#&gt; [1] 0.42\n\n\n\n\\(P(\\overline{T}\\cap S) = P(S) * P(\\overline{T}/S)\\)\n\n\nP_not_T_and_S &lt;- 0.3 * 0.8\nP_not_T_and_S\n\n#&gt; [1] 0.24\n\n\n\n\\(P(\\overline{T}\\cap \\overline{S}) = P(\\overline{S}) * P(\\overline{T}/\\overline{S})\\)\n\n\nP_not_T_and_S_not &lt;- 0.7 * 0.4\nP_not_T_and_S_not\n\n#&gt; [1] 0.28\n\n\n\nSum of all probabilities\n\n\nsum &lt;- P_T_and_S + P_T_and_S_not + P_not_T_and_S + P_not_T_and_S_not\nsum\n\n#&gt; [1] 1\n\n\n\n\nAssignment 2\n\n\nThe percentage of customers using all three devices = 0.5%\nThe percentage of customers using at least two devices = 7.3 + 3.3 + 8.8 + 0.5 = 19.9%\nThe percentage of customers using only one device = 42.3 + 27.8 + 10 = 80.1%\n\n\n\nAssignment 3\n\n\nGiven Probabilities - \\(P(B|A)=0.97\\)\n\\(P(B|\\overline{A})=0.01\\)\n\\(P(A)=0.04\\)\n\nBy Bayesian Theorem, \\(P(A|B) = \\frac{P(A|B)*P(A)}{P(B)} = \\frac{P(A|B)*P(A)}{P(B|A)*P(A)+P(B|\\overline{A})*P(\\overline{A})}\\)\n\nProbability that the product is not faulty\n\n\n\\(P(\\overline{A}|B)\\)\n\n\nP_A_not_given_B = (0.01*0.96)/((0.01*0.96)+(0.97*0.04))\nP_A_not_given_B_percentage = P_A_not_given_B * 100\n\npaste(\"Probability that the product is not faulty -\", P_A_not_given_B_percentage, \"%\")\n\n#&gt; [1] \"Probability that the product is not faulty - 19.8347107438017 %\"\n\n\n\nProbability that the product is faulty\n\n\n\\(P(A|B)\\)\n\n\nP_A_given_B = (0.97*0.04)/((0.01*0.96)+(0.97*0.04))\nP_A_given_B_percentage = P_A_given_B * 100\n\npaste(\"Probability that the product is faulty -\", P_A_given_B_percentage, \"%\")\n\n#&gt; [1] \"Probability that the product is faulty - 80.1652892561983 %\"\n\n\nThese results show that in case the alarm is triggered, there is a possibility of about 19.83% that the product is flawless and a probability of 80.17% that the product is faulty."
  },
  {
    "objectID": "content/01_journal/01_probability.html#header-2",
    "href": "content/01_journal/01_probability.html#header-2",
    "title": "Probability Theory",
    "section": "5.1 Header 2",
    "text": "5.1 Header 2\n\nHeader 3\n\nHeader 4\n\nHeader 5\n\nHeader 6"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "Task 1\n\n\nLoading the data set\n\n\ndf &lt;- readRDS(\"Causal_Data_Science_Data/car_prices.rds\")\n\n\nChecking the dimensions of the dataset\n\n\ndimensions &lt;- dim(df)\npaste(\"Dimensions of the dataset =\", dimensions)\n\n#&gt; [1] \"Dimensions of the dataset = 181\" \"Dimensions of the dataset = 22\"\n\n\n\n\nTask 2\n\n\nhead(df)\n\n\n\n  \n\n\n# Show table\ndf\n\n\n\n  \n\n\n# Get glimpse of data\nglimpse(df)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\nsummary(df, digits = 3)\n\n#&gt;   aspiration         doornumber          carbody           drivewheel       \n#&gt;  Length:181         Length:181         Length:181         Length:181        \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;  enginelocation       wheelbase       carlength      carwidth      carheight   \n#&gt;  Length:181         Min.   : 86.6   Min.   :141   Min.   :60.3   Min.   :47.8  \n#&gt;  Class :character   1st Qu.: 94.5   1st Qu.:166   1st Qu.:64.0   1st Qu.:52.0  \n#&gt;  Mode  :character   Median : 96.5   Median :173   Median :65.4   Median :53.7  \n#&gt;                     Mean   : 98.2   Mean   :173   Mean   :65.7   Mean   :53.6  \n#&gt;                     3rd Qu.:100.4   3rd Qu.:180   3rd Qu.:66.5   3rd Qu.:55.5  \n#&gt;                     Max.   :120.9   Max.   :208   Max.   :72.3   Max.   :59.8  \n#&gt;    curbweight    enginetype        cylindernumber       enginesize \n#&gt;  Min.   :1488   Length:181         Length:181         Min.   : 61  \n#&gt;  1st Qu.:2122   Class :character   Class :character   1st Qu.: 98  \n#&gt;  Median :2410   Mode  :character   Mode  :character   Median :120  \n#&gt;  Mean   :2521                                         Mean   :127  \n#&gt;  3rd Qu.:2910                                         3rd Qu.:141  \n#&gt;  Max.   :4066                                         Max.   :326  \n#&gt;   fuelsystem          boreratio        stroke     compressionratio\n#&gt;  Length:181         Min.   :2.54   Min.   :2.07   Min.   : 7.00   \n#&gt;  Class :character   1st Qu.:3.15   1st Qu.:3.08   1st Qu.: 8.50   \n#&gt;  Mode  :character   Median :3.31   Median :3.23   Median : 9.00   \n#&gt;                     Mean   :3.32   Mean   :3.23   Mean   : 8.85   \n#&gt;                     3rd Qu.:3.59   3rd Qu.:3.40   3rd Qu.: 9.40   \n#&gt;                     Max.   :3.94   Max.   :4.17   Max.   :11.50   \n#&gt;    horsepower     peakrpm        citympg       highwaympg       price      \n#&gt;  Min.   : 48   Min.   :4200   Min.   :13.0   Min.   :16.0   Min.   : 5118  \n#&gt;  1st Qu.: 70   1st Qu.:4800   1st Qu.:19.0   1st Qu.:25.0   1st Qu.: 7609  \n#&gt;  Median : 95   Median :5200   Median :24.0   Median :30.0   Median : 9980  \n#&gt;  Mean   :106   Mean   :5182   Mean   :24.8   Mean   :30.5   Mean   :12999  \n#&gt;  3rd Qu.:116   3rd Qu.:5500   3rd Qu.:30.0   3rd Qu.:34.0   3rd Qu.:16430  \n#&gt;  Max.   :288   Max.   :6600   Max.   :49.0   Max.   :54.0   Max.   :45400\n\n# Select only numeric columns\ndf_numeric &lt;- df[sapply(df, is.numeric)]\n\n# Calculate correlation matrix\ncor(df_numeric)\n\n#&gt;                    wheelbase   carlength   carwidth   carheight curbweight\n#&gt; wheelbase         1.00000000  0.86117385  0.7697984  0.54019069  0.7367123\n#&gt; carlength         0.86117385  1.00000000  0.8264703  0.44081316  0.8660053\n#&gt; carwidth          0.76979836  0.82647026  1.0000000  0.20488138  0.8480000\n#&gt; carheight         0.54019069  0.44081316  0.2048814  1.00000000  0.2141918\n#&gt; curbweight        0.73671231  0.86600528  0.8480000  0.21419176  1.0000000\n#&gt; enginesize        0.55411851  0.68017071  0.7431094 -0.02315796  0.8664640\n#&gt; boreratio         0.46436865  0.60094354  0.5515756  0.13853090  0.6382900\n#&gt; stroke            0.07182428  0.06571525  0.1143203 -0.15074728  0.1003967\n#&gt; compressionratio -0.25528942 -0.25098987 -0.2497246 -0.04784995 -0.3108351\n#&gt; horsepower        0.40436893  0.59890956  0.6971702 -0.09471443  0.8176250\n#&gt; peakrpm          -0.21992786 -0.19115857 -0.1086424 -0.15338356 -0.1596062\n#&gt; citympg          -0.58337119 -0.78122590 -0.7392601 -0.15044099 -0.8727123\n#&gt; highwaympg       -0.62686055 -0.78886402 -0.7501573 -0.18342575 -0.8873144\n#&gt; price             0.55547499  0.67307358  0.7430132  0.07452058  0.8340814\n#&gt;                   enginesize  boreratio      stroke compressionratio\n#&gt; wheelbase         0.55411851  0.4643687  0.07182428      -0.25528942\n#&gt; carlength         0.68017071  0.6009435  0.06571525      -0.25098987\n#&gt; carwidth          0.74310940  0.5515756  0.11432033      -0.24972464\n#&gt; carheight        -0.02315796  0.1385309 -0.15074728      -0.04784995\n#&gt; curbweight        0.86646404  0.6382900  0.10039673      -0.31083510\n#&gt; enginesize        1.00000000  0.5779073  0.18198111      -0.16132409\n#&gt; boreratio         0.57790729  1.0000000 -0.10077211      -0.19600019\n#&gt; stroke            0.18198111 -0.1007721  1.00000000      -0.30056815\n#&gt; compressionratio -0.16132409 -0.1960002 -0.30056815       1.00000000\n#&gt; horsepower        0.84661455  0.5917398  0.11169001      -0.22352509\n#&gt; peakrpm          -0.18477648 -0.2363817  0.06779511       0.15590249\n#&gt; citympg          -0.74373969 -0.6185363 -0.09158746       0.43789655\n#&gt; highwaympg       -0.75541425 -0.6086412 -0.07422008       0.44866322\n#&gt; price             0.88970342  0.5540692  0.03082326      -0.17646152\n#&gt;                   horsepower     peakrpm     citympg  highwaympg       price\n#&gt; wheelbase         0.40436893 -0.21992786 -0.58337119 -0.62686055  0.55547499\n#&gt; carlength         0.59890956 -0.19115857 -0.78122590 -0.78886402  0.67307358\n#&gt; carwidth          0.69717017 -0.10864244 -0.73926008 -0.75015733  0.74301316\n#&gt; carheight        -0.09471443 -0.15338356 -0.15044099 -0.18342575  0.07452058\n#&gt; curbweight        0.81762499 -0.15960618 -0.87271235 -0.88731441  0.83408139\n#&gt; enginesize        0.84661455 -0.18477648 -0.74373969 -0.75541425  0.88970342\n#&gt; boreratio         0.59173978 -0.23638175 -0.61853627 -0.60864116  0.55406923\n#&gt; stroke            0.11169001  0.06779511 -0.09158746 -0.07422008  0.03082326\n#&gt; compressionratio -0.22352509  0.15590249  0.43789655  0.44866322 -0.17646152\n#&gt; horsepower        1.00000000  0.08277528 -0.80612428 -0.77717065  0.84009827\n#&gt; peakrpm           0.08277528  1.00000000  0.02610522  0.05111792 -0.02085359\n#&gt; citympg          -0.80612428  0.02610522  1.00000000  0.97537685 -0.74198802\n#&gt; highwaympg       -0.77717065  0.05111792  0.97537685  1.00000000 -0.74079733\n#&gt; price             0.84009827 -0.02085359 -0.74198802 -0.74079733  1.00000000\n\n\n\nTo view the data type in the DataFrame.\n\n\nstr(df)\n\n#&gt; tibble [181 × 22] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ aspiration      : chr [1:181] \"std\" \"std\" \"std\" \"std\" ...\n#&gt;  $ doornumber      : chr [1:181] \"two\" \"two\" \"two\" \"four\" ...\n#&gt;  $ carbody         : chr [1:181] \"convertible\" \"convertible\" \"hatchback\" \"sedan\" ...\n#&gt;  $ drivewheel      : chr [1:181] \"rwd\" \"rwd\" \"rwd\" \"fwd\" ...\n#&gt;  $ enginelocation  : chr [1:181] \"front\" \"front\" \"front\" \"front\" ...\n#&gt;  $ wheelbase       : num [1:181] 88.6 88.6 94.5 99.8 99.4 ...\n#&gt;  $ carlength       : num [1:181] 169 169 171 177 177 ...\n#&gt;  $ carwidth        : num [1:181] 64.1 64.1 65.5 66.2 66.4 66.3 71.4 71.4 71.4 67.9 ...\n#&gt;  $ carheight       : num [1:181] 48.8 48.8 52.4 54.3 54.3 53.1 55.7 55.7 55.9 52 ...\n#&gt;  $ curbweight      : num [1:181] 2548 2548 2823 2337 2824 ...\n#&gt;  $ enginetype      : chr [1:181] \"dohc\" \"dohc\" \"ohcv\" \"ohc\" ...\n#&gt;  $ cylindernumber  : chr [1:181] \"four\" \"four\" \"six\" \"four\" ...\n#&gt;  $ enginesize      : num [1:181] 130 130 152 109 136 136 136 136 131 131 ...\n#&gt;  $ fuelsystem      : chr [1:181] \"mpfi\" \"mpfi\" \"mpfi\" \"mpfi\" ...\n#&gt;  $ boreratio       : num [1:181] 3.47 3.47 2.68 3.19 3.19 3.19 3.19 3.19 3.13 3.13 ...\n#&gt;  $ stroke          : num [1:181] 2.68 2.68 3.47 3.4 3.4 3.4 3.4 3.4 3.4 3.4 ...\n#&gt;  $ compressionratio: num [1:181] 9 9 9 10 8 8.5 8.5 8.5 8.3 7 ...\n#&gt;  $ horsepower      : num [1:181] 111 111 154 102 115 110 110 110 140 160 ...\n#&gt;  $ peakrpm         : num [1:181] 5000 5000 5000 5500 5500 5500 5500 5500 5500 5500 ...\n#&gt;  $ citympg         : num [1:181] 21 21 19 24 18 19 19 19 17 16 ...\n#&gt;  $ highwaympg      : num [1:181] 27 27 26 30 22 25 25 25 20 22 ...\n#&gt;  $ price           : num [1:181] 13495 16500 16500 13950 17450 ...\n\n\n\nWe have “chr” and “num” as data types in our dataset.\n\n\nNumbers and strings are different data types and they are treated differently.\n\n\nNumbers: These are numerical values that can be integers or doubles (for decimal numbers). You can perform arithmetic operations on numbers. For example, you can define a number as num &lt;- 10 or num &lt;- 10.5.\nStrings (Character): These are sequences of characters. Strings are used to store text data. You can perform operations like concatenation on strings, but you cannot perform arithmetic operations. You can define a string as str &lt;- “Hello, World!”\n\n\n\nTask 3\n\n\nlm_model &lt;- lm(price ~ ., data = df)\n\nsummary(lm_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ ., data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients:\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nThe more ’*’ there are, the more important the factors are for the car’s price. Looking at the table, we can say these things really matter when it comes to deciding how much the car costs.\n\n\nTask 4\n\nI would like to select peakrpm as a Regressor.\n\nData Type: As observed in the linear regression summary, the peakrpm variable is of numeric type.\nImpact on Price: The coefficient for peakrpm is 2.526. This signifies that for every one-unit increase in size, the expected price increase is 2.526 units, while holding other variables constant.\nStatistical Significance: The p-value is remarkably low at 0.000108, suggesting high statistical significance. This implies that the influence of peakrpm on the Price is substantial.\n\n\n\nTask 5\n\n\nlibrary(dplyr)\n\ndf &lt;- mutate(df, seat_heating = TRUE)\n\nmodel &lt;- lm(price ~ . + seat_heating, data = df)\n\nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ . + seat_heating, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -5662  -1120      0    798   9040 \n#&gt; \n#&gt; Coefficients: (1 not defined because of singularities)\n#&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)          -36269.965  15460.866  -2.346 0.020354 *  \n#&gt; aspirationturbo        1846.206   1041.391   1.773 0.078386 .  \n#&gt; doornumbertwo           242.523    571.929   0.424 0.672172    \n#&gt; carbodyhardtop        -3691.743   1424.825  -2.591 0.010561 *  \n#&gt; carbodyhatchback      -3344.335   1238.359  -2.701 0.007757 ** \n#&gt; carbodysedan          -2292.820   1356.014  -1.691 0.093043 .  \n#&gt; carbodywagon          -3427.921   1490.285  -2.300 0.022885 *  \n#&gt; drivewheelfwd          -504.564   1076.623  -0.469 0.640030    \n#&gt; drivewheelrwd           -15.446   1268.070  -0.012 0.990299    \n#&gt; enginelocationrear     6643.492   2572.275   2.583 0.010806 *  \n#&gt; wheelbase               -30.197     92.776  -0.325 0.745294    \n#&gt; carlength               -29.740     51.672  -0.576 0.565824    \n#&gt; carwidth                731.819    244.533   2.993 0.003258 ** \n#&gt; carheight               123.195    134.607   0.915 0.361617    \n#&gt; curbweight                2.612      1.781   1.467 0.144706    \n#&gt; enginetypedohcv       -8541.957   4749.685  -1.798 0.074219 .  \n#&gt; enginetypel             978.748   1786.384   0.548 0.584619    \n#&gt; enginetypeohc          3345.252    933.001   3.585 0.000461 ***\n#&gt; enginetypeohcf          972.919   1625.631   0.598 0.550462    \n#&gt; enginetypeohcv        -6222.322   1236.415  -5.033 1.43e-06 ***\n#&gt; cylindernumberfive   -11724.540   3019.192  -3.883 0.000157 ***\n#&gt; cylindernumberfour   -11549.326   3177.177  -3.635 0.000387 ***\n#&gt; cylindernumbersix     -7151.398   2247.230  -3.182 0.001793 ** \n#&gt; cylindernumberthree   -4318.929   4688.833  -0.921 0.358545    \n#&gt; cylindernumbertwelve -11122.209   4196.494  -2.650 0.008946 ** \n#&gt; enginesize              125.934     26.541   4.745 5.00e-06 ***\n#&gt; fuelsystem2bbl          177.136    883.615   0.200 0.841400    \n#&gt; fuelsystemmfi         -3041.018   2576.996  -1.180 0.239934    \n#&gt; fuelsystemmpfi          359.278   1001.529   0.359 0.720326    \n#&gt; fuelsystemspdi        -2543.890   1363.546  -1.866 0.064140 .  \n#&gt; fuelsystemspfi          514.766   2499.229   0.206 0.837107    \n#&gt; boreratio             -1306.740   1642.221  -0.796 0.427516    \n#&gt; stroke                -4527.137    922.732  -4.906 2.49e-06 ***\n#&gt; compressionratio       -737.901    555.960  -1.327 0.186539    \n#&gt; horsepower               10.293     22.709   0.453 0.651035    \n#&gt; peakrpm                   2.526      0.634   3.983 0.000108 ***\n#&gt; citympg                 -90.352    166.647  -0.542 0.588538    \n#&gt; highwaympg              154.858    167.148   0.926 0.355761    \n#&gt; seat_heatingTRUE             NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2189 on 143 degrees of freedom\n#&gt; Multiple R-squared:  0.9415, Adjusted R-squared:  0.9264 \n#&gt; F-statistic: 62.21 on 37 and 143 DF,  p-value: &lt; 2.2e-16\n\n\nSince seat_heating is a binary variable with the same value for all observations, it will not have any predictive power in the model. The coefficient for seat_heating will be NA, and the t-value and p-value will also be NA. This is because a variable with a single value does not provide any information that can be used to predict the dependent variable."
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "Loading the data frame\nlibrary(dplyr)\n\ndf_coupon &lt;- readRDS(\"Causal_Data_Science_Data/coupon.rds\")\n\ndf_coupon"
  },
  {
    "objectID": "content/01_journal/05_dag.html#regress-satisfaction-on-follow_ups",
    "href": "content/01_journal/05_dag.html#regress-satisfaction-on-follow_ups",
    "title": "Directed Acyclic Graphs",
    "section": "1. Regress satisfaction on follow_ups",
    "text": "1. Regress satisfaction on follow_ups\n\nlm_model &lt;- lm(satisfaction ~ follow_ups, data = df)\n\nsummary(lm_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427"
  },
  {
    "objectID": "content/01_journal/05_dag.html#regress-satisfaction-on-follow_ups-and-account-for-subscription",
    "href": "content/01_journal/05_dag.html#regress-satisfaction-on-follow_ups-and-account-for-subscription",
    "title": "Directed Acyclic Graphs",
    "section": "2. Regress satisfaction on follow_ups and account for subscription",
    "text": "2. Regress satisfaction on follow_ups and account for subscription\n\nlm_model_subscription &lt;- lm(satisfaction ~ ., data = df)\n\nsummary(lm_model_subscription)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ ., data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08"
  },
  {
    "objectID": "content/01_journal/05_dag.html#without-conditioning-on-subscription",
    "href": "content/01_journal/05_dag.html#without-conditioning-on-subscription",
    "title": "Directed Acyclic Graphs",
    "section": "Without Conditioning on Subscription",
    "text": "Without Conditioning on Subscription\n\nsimps_not_cond  &lt;- ggplot(df, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F)\n\nsimps_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/05_dag.html#conditioning-on-subscription",
    "href": "content/01_journal/05_dag.html#conditioning-on-subscription",
    "title": "Directed Acyclic Graphs",
    "section": "Conditioning on Subscription",
    "text": "Conditioning on Subscription\n\nsimps_cond &lt;- ggplot(df, aes(x = follow_ups, y = satisfaction, color = subscription )) +\n  geom_point(alpha = .8) +\n  stat_smooth(method = \"lm\", se = F) +\n  theme(legend.position = \"right\")\n\nsimps_cond \n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/07_matching.html#loading-the-data-frame",
    "href": "content/01_journal/07_matching.html#loading-the-data-frame",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "df  &lt;- readRDS(\"Causal_Data_Science_Data/membership.rds\")\ndf"
  },
  {
    "objectID": "content/01_journal/07_matching.html#dag",
    "href": "content/01_journal/07_matching.html#dag",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\n\n# Define the DAG\ndag &lt;- 'dag {\n  card [pos=\"0.1,0.5\"]\n  avg_purch [pos=\"0.5,0.5\"]\n  pre_avg_purch [pos=\"0.3,1\"]\n  pre_avg_purch -&gt; card\n  pre_avg_purch -&gt; avg_purch\n  card -&gt; avg_purch\n}'\n\n# Convert the text to a dagitty object\ndag &lt;- dagitty(dag)\n\n# Plot the DAG using ggdag\nggdag(dag) + theme_dag() + geom_dag_label_repel(aes(label = name))"
  },
  {
    "objectID": "content/01_journal/07_matching.html#apply-coarsed-matching-method",
    "href": "content/01_journal/07_matching.html#apply-coarsed-matching-method",
    "title": "Matching and Subclassification",
    "section": "Apply Coarsed Matching Method",
    "text": "Apply Coarsed Matching Method\n\nlibrary(MatchIt)\nmatch_cem &lt;- matchit(card ~ sex + age + pre_avg_purch, data = df, method = \"cem\")\nsummary(match_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ sex + age + pre_avg_purch, data = df, \n#&gt;     method = \"cem\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; sex             0.0086\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5070        0.5070          0.0000          .    0.0000\n#&gt; age                 41.6786       41.6527          0.0019     0.9995    0.0016\n#&gt; pre_avg_purch       75.7076       75.3560          0.0134     0.9951    0.0042\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; sex             0.0000          0.0000\n#&gt; age             0.0070          0.1181\n#&gt; pre_avg_purch   0.0128          0.1547\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            5768.     4232\n#&gt; Matched (ESS)  4407.4    4164\n#&gt; Matched        5716.     4164\n#&gt; Unmatched        52.       68\n#&gt; Discarded         0.        0\n\ndata_cem &lt;- match.data(match_cem)\nsummary(data_cem)\n\n#&gt;       age             sex        pre_avg_purch          card       \n#&gt;  Min.   :16.00   Min.   :0.000   Min.   : -1.791   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.000   1st Qu.: 51.859   1st Qu.:0.0000  \n#&gt;  Median :38.60   Median :1.000   Median : 69.983   Median :0.0000  \n#&gt;  Mean   :40.17   Mean   :0.504   Mean   : 70.242   Mean   :0.4215  \n#&gt;  3rd Qu.:48.90   3rd Qu.:1.000   3rd Qu.: 88.375   3rd Qu.:1.0000  \n#&gt;  Max.   :84.60   Max.   :1.000   Max.   :153.713   Max.   :1.0000  \n#&gt;                                                                    \n#&gt;    avg_purch         weights           subclass   \n#&gt;  Min.   :-28.61   Min.   : 0.1615   5      : 148  \n#&gt;  1st Qu.: 54.08   1st Qu.: 0.8527   94     : 142  \n#&gt;  Median : 76.11   Median : 1.0000   4      : 141  \n#&gt;  Mean   : 76.39   Mean   : 1.0000   80     : 140  \n#&gt;  3rd Qu.: 98.06   3rd Qu.: 1.0000   54     : 136  \n#&gt;  Max.   :192.91   Max.   :10.9817   29     : 135  \n#&gt;                                     (Other):9038\n\nmodel_cem &lt;- lm(avg_purch ~ card, data = data_cem)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = data_cem)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -100.835  -20.456   -0.178   20.041  119.973 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  66.1326     0.3934  168.11   &lt;2e-16 ***\n#&gt; card         24.3461     0.6060   40.18   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 29.74 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.1405, Adjusted R-squared:  0.1404 \n#&gt; F-statistic:  1614 on 1 and 9878 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#apply-coarsed-matching-method-with-curpoints",
    "href": "content/01_journal/07_matching.html#apply-coarsed-matching-method-with-curpoints",
    "title": "Matching and Subclassification",
    "section": "Apply Coarsed Matching Method with curpoints",
    "text": "Apply Coarsed Matching Method with curpoints\n\nlibrary(MatchIt)\ncutpoints &lt;- list(age = seq(16, 85, 10), \n                  pre_avg_purch = seq(0, 160, 20))\nmatch_cem &lt;- matchit(card ~ sex + age + pre_avg_purch, data = df, method = \"cem\", cutpoints = cutpoints)\nsummary(match_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ sex + age + pre_avg_purch, data = df, \n#&gt;     method = \"cem\", cutpoints = cutpoints)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; sex             0.0086\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5095        0.5095          0.0000          .    0.0000\n#&gt; age                 41.9593       41.8825          0.0055     1.0203    0.0024\n#&gt; pre_avg_purch       76.1990       75.5617          0.0242     1.0038    0.0064\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; sex             0.0000          0.0000\n#&gt; age             0.0092          0.2317\n#&gt; pre_avg_purch   0.0185          0.2486\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 4704.04    4214\n#&gt; Matched       5750.      4214\n#&gt; Unmatched       18.        18\n#&gt; Discarded        0.         0\n\ndata_cem &lt;- match.data(match_cem)\nsummary(data_cem)\n\n#&gt;       age             sex         pre_avg_purch           card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :  0.3307   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.8799   1st Qu.:0.0000  \n#&gt;  Median :38.70   Median :1.0000   Median : 70.1684   Median :0.0000  \n#&gt;  Mean   :40.34   Mean   :0.5036   Mean   : 70.4465   Mean   :0.4229  \n#&gt;  3rd Qu.:49.10   3rd Qu.:1.0000   3rd Qu.: 88.6960   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :158.5457   Max.   :1.0000  \n#&gt;                                                                      \n#&gt;    avg_purch         weights          subclass   \n#&gt;  Min.   :-28.61   Min.   :0.1516   9      : 422  \n#&gt;  1st Qu.: 54.11   1st Qu.:0.8676   2      : 419  \n#&gt;  Median : 76.27   Median :1.0000   1      : 415  \n#&gt;  Mean   : 76.64   Mean   :1.0000   4      : 407  \n#&gt;  3rd Qu.: 98.47   3rd Qu.:1.0000   32     : 405  \n#&gt;  Max.   :192.91   Max.   :6.1402   45     : 392  \n#&gt;                                    (Other):7504\n\nmodel_cem &lt;- lm(avg_purch ~ card, data = data_cem)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = data_cem)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.330  -20.623   -0.241   20.317  119.975 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  66.1307     0.3946  167.58   &lt;2e-16 ***\n#&gt; card         24.8434     0.6068   40.94   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 29.92 on 9962 degrees of freedom\n#&gt; Multiple R-squared:  0.144,  Adjusted R-squared:  0.1439 \n#&gt; F-statistic:  1676 on 1 and 9962 DF,  p-value: &lt; 2.2e-16\n\n\n\nAdding cutpoints, did not seem to make much difference to the balance between the treatment and the control group indicating the treatment effect between the treatment and the control group. But this could also be due the the selection of the cutpoints. In this case, it was chosen to be age and pre_avg_purch might not have strong impact on the df program."
  },
  {
    "objectID": "content/01_journal/07_matching.html#apply-nearest-neighbour-matching-method",
    "href": "content/01_journal/07_matching.html#apply-nearest-neighbour-matching-method",
    "title": "Matching and Subclassification",
    "section": "Apply nearest neighbour matching Method",
    "text": "Apply nearest neighbour matching Method\n\nmatch_nn &lt;- matchit(card ~ age + sex + pre_avg_purch, data = df, method = \"nearest\", distance = \"mahalanobis\", replace = T)\nsummary(match_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + sex + pre_avg_purch, data = df, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\ndata_nn &lt;- match.data(match_nn)\nsummary(data_nn)\n\n#&gt;       age             sex         pre_avg_purch          card       \n#&gt;  Min.   :16.20   Min.   :0.0000   Min.   : -3.966   Min.   :0.0000  \n#&gt;  1st Qu.:30.80   1st Qu.:0.0000   1st Qu.: 56.368   1st Qu.:0.0000  \n#&gt;  Median :39.90   Median :1.0000   Median : 74.441   Median :1.0000  \n#&gt;  Mean   :41.51   Mean   :0.5086   Mean   : 74.705   Mean   :0.6125  \n#&gt;  3rd Qu.:50.60   3rd Qu.:1.0000   3rd Qu.: 92.900   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.416   Max.   :1.0000  \n#&gt;    avg_purch         weights      \n#&gt;  Min.   :-23.40   Min.   :0.6326  \n#&gt;  1st Qu.: 62.22   1st Qu.:1.0000  \n#&gt;  Median : 83.73   Median :1.0000  \n#&gt;  Mean   : 83.79   Mean   :1.0000  \n#&gt;  3rd Qu.:105.06   3rd Qu.:1.0000  \n#&gt;  Max.   :192.91   Max.   :5.0605\n\nmodel_nn &lt;- lm(avg_purch ~ card, data = data_nn)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = data_nn)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.666   -0.065   20.493  113.958 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  72.1472     0.5807  124.24   &lt;2e-16 ***\n#&gt; card         19.0120     0.7420   25.62   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.05 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.0868, Adjusted R-squared:  0.08667 \n#&gt; F-statistic: 656.5 on 1 and 6907 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#apply-inverse-probability-weightning-matching-method",
    "href": "content/01_journal/07_matching.html#apply-inverse-probability-weightning-matching-method",
    "title": "Matching and Subclassification",
    "section": "Apply inverse probability weightning matching Method",
    "text": "Apply inverse probability weightning matching Method\n\npropensity_model &lt;- glm(card ~ age + sex + pre_avg_purch, family = binomial(link = \"logit\"), data = df)\nsummary(propensity_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + sex + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\ndf$propensity &lt;- predict(propensity_model, type = \"response\")\nsummary(df$propensity)\n\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.1705  0.3534  0.4192  0.4232  0.4886  0.7695\n\ndf$ipw &lt;- with(df, (card / propensity) + ((1 - card) / (1 - propensity)))\nsummary(df$ipw)\n\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;   1.206   1.611   1.871   2.000   2.256   5.308\n\nmodel_ipw &lt;- lm(avg_purch ~ card, data = df, weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#coarsened-exact-matching.",
    "href": "content/01_journal/07_matching.html#coarsened-exact-matching.",
    "title": "Matching and Subclassification",
    "section": "(Coarsened) Exact Matching.",
    "text": "(Coarsened) Exact Matching.\n\nWithout specifying coarsening\n\nlibrary(MatchIt)\ncem &lt;- matchit(card ~ sex + age + pre_avg_purch, data = df, method = \"cem\", estimand = 'ATE')\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ sex + age + pre_avg_purch, data = df, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; sex             0.0086\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5040        0.5040          0.0000          .    0.0000\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; sex             0.0000          0.0000\n#&gt; age             0.0064          0.1222\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\ndf_cem &lt;- match.data(cem)\n\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCustom coarsening\n\ncutpoints &lt;- list(age = seq(24, 65, 18), \n                  pre_avg_purch = seq(3, 200, 50))\ncem_coars &lt;- matchit(card ~ sex + age + pre_avg_purch, data = df, method = \"cem\", estimand = 'ATE', cutpoints = cutpoints)\nsummary(cem_coars)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ sex + age + pre_avg_purch, data = df, \n#&gt;     method = \"cem\", estimand = \"ATE\", cutpoints = cutpoints)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; sex             0.0086\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; sex                  0.5035        0.5035          0.0000          .    0.0000\n#&gt; age                 40.4546       40.2862          0.0125     1.0210    0.0035\n#&gt; pre_avg_purch       71.6914       69.4008          0.0877     1.0108    0.0233\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; sex             0.0000          0.0000\n#&gt; age             0.0111          0.4227\n#&gt; pre_avg_purch   0.0533          0.5663\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.   4232.  \n#&gt; Matched (ESS) 5632.85 4036.73\n#&gt; Matched       5768.   4226.  \n#&gt; Unmatched        0.      6.  \n#&gt; Discarded        0.      0.\n\ndf_cem &lt;- match.data(cem_coars)\n\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -131.552  -20.754   -0.009   20.284  135.801 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.2933     0.3996   173.4   &lt;2e-16 ***\n#&gt; card         17.1468     0.6146    27.9   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.35 on 9992 degrees of freedom\n#&gt; Multiple R-squared:  0.07228,    Adjusted R-squared:  0.07218 \n#&gt; F-statistic: 778.5 on 1 and 9992 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#nearest-neighbor-matching.",
    "href": "content/01_journal/07_matching.html#nearest-neighbor-matching.",
    "title": "Matching and Subclassification",
    "section": "Nearest-Neighbor Matching.",
    "text": "Nearest-Neighbor Matching.\n\nnn &lt;- matchit(card ~ age + sex + pre_avg_purch, data = df, method = \"nearest\", distance = \"mahalanobis\", replace = T)\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + sex + pre_avg_purch, data = df, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; sex             0.0086\n#&gt; pre_avg_purch   0.1545\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; sex             0.0000          0.0000\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\ndf_nn &lt;- match.data(nn)\n\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#inverse-probability-weighting.",
    "href": "content/01_journal/07_matching.html#inverse-probability-weighting.",
    "title": "Matching and Subclassification",
    "section": "Inverse Probability Weighting.",
    "text": "Inverse Probability Weighting.\n\nmodel_prop &lt;- glm(card ~ age + sex + pre_avg_purch, data = df, family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + sex + pre_avg_purch, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\ndf_aug &lt;- df %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\ndf_ipw &lt;- df_aug %&gt;% mutate(ipw = (card / propensity) + ((1 - card) / (1 - propensity)))\n\ndf_ipw %&gt;% \n  select(card, age, sex, pre_avg_purch, propensity, ipw)\n\n\n\n  \n\n\nmodel_ipw &lt;- lm(avg_purch ~ card, data = df_ipw, weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/09_iv.html#draw-dag",
    "href": "content/01_journal/09_iv.html#draw-dag",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(ggplot2)\n\n\niv_expl &lt;- dagify(\n  Y ~ D,\n  Y ~ U,\n  D ~ U,\n  D ~ Z,\n  exposure = \"D\",\n  latent = \"U\",\n  outcome = \"Y\",\n  coords = list(x = c(U = 1, D = 0, Y = 2, Z = -1),\n                y = c(U = 1, D = 0, Y = 0, Z = 0)),\n  labels = c(\"D\" = \"Pop-up Feature\", \n             \"Y\" = \"App Usage\", \n             \"U\" = \"Unobserved Factor\",\n             \"Z\" = \"Encouragement\")\n)\nggdag(iv_expl, text = T) +\n  guides(color = \"none\") +\n  theme_minimal() + \n  geom_dag_point() +\n  geom_dag_text() +\n  geom_dag_edges() +\n  geom_dag_label_repel(aes(label = label))"
  },
  {
    "objectID": "content/01_journal/09_iv.html#naive-based-estimate",
    "href": "content/01_journal/09_iv.html#naive-based-estimate",
    "title": "Instrumental Variables",
    "section": "",
    "text": "df &lt;- readRDS(\"Causal_Data_Science_Data/rand_enc.rds\")\ndf\n\n\n\n  \n\n\n\n\nmodel_biased &lt;- lm(time_spent ~ used_ftr, data = df)\nsummary(model_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/09_iv.html#test-the-assumption-encouragement",
    "href": "content/01_journal/09_iv.html#test-the-assumption-encouragement",
    "title": "Instrumental Variables",
    "section": "",
    "text": "cor(df$rand_enc, df$used_ftr)\n\n#&gt; [1] 0.204411\n\ncor(df$rand_enc, df$time_spent)\n\n#&gt; [1] 0.1296716\n\n\n\nThere seems to be a strong naive estimate relationship between the pop feature usage and the app time, based on the calculations from above.\nAdditionally, there seems to be a positive relationship between the random encouragement and the pop feature, suggesting that the instrument (random encouragement) was relevant. Similarly, there seems to be a relationship as well between the random encouragement and the time spent, but it’s not as significant as the former correlation. This can also be validated, as the random encouragement should not have a significant impact on the time spent by the users on the application."
  },
  {
    "objectID": "content/01_journal/09_iv.html#estimate-using-2sls",
    "href": "content/01_journal/09_iv.html#estimate-using-2sls",
    "title": "Instrumental Variables",
    "section": "",
    "text": "library(AER)\niv_estimate &lt;- ivreg(time_spent ~ used_ftr | rand_enc, data = df)\nsummary(iv_estimate)\n\n#&gt; \n#&gt; Call:\n#&gt; ivreg(formula = time_spent ~ used_ftr | rand_enc, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -20.937  -3.622   0.061   3.568  20.063 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  19.3124     0.2249   85.86   &lt;2e-16 ***\n#&gt; used_ftr      9.7382     0.5353   18.19   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.377 on 9998 degrees of freedom\n#&gt; Multiple R-Squared: 0.4921,  Adjusted R-squared: 0.492 \n#&gt; Wald test:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe Naive estimate is 10.82, while the IV estimate is 9.73. The naive estimate shows an upward bias as it does not account for unobserved confounders that are related to the feature use and the time spent on the application which is the encouragement. While the IV estimates do take this into account, therefore the estimate was slightly lower. The naive estimate captures not just the effect of the feature usage on the time spent on the application, but also the encouragement, which leads to an overestimate."
  },
  {
    "objectID": "content/01_journal/09_iv.html#task---2",
    "href": "content/01_journal/09_iv.html#task---2",
    "title": "Instrumental Variables",
    "section": "",
    "text": "df &lt;- readRDS(\"Causal_Data_Science_Data/rand_enc.rds\")\ndf\n\n\n\n  \n\n\n\n\nmodel_biased &lt;- lm(time_spent ~ used_ftr, data = df)\nsummary(model_biased)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/09_iv.html#task---3",
    "href": "content/01_journal/09_iv.html#task---3",
    "title": "Instrumental Variables",
    "section": "",
    "text": "cor(df) %&gt;% round(2)\n\n#&gt;            rand_enc used_ftr time_spent\n#&gt; rand_enc       1.00     0.20       0.13\n#&gt; used_ftr       0.20     1.00       0.71\n#&gt; time_spent     0.13     0.71       1.00\n\n\n\nThere seems to be a strong naive estimate relationship between the pop feature usage and the app time, based on the calculations from above.\nAdditionally, there seems to be a positive relationship between the random encouragement and the pop feature, suggesting that the instrument (random encouragement) was relevant. Similarly, there seems to be a relationship as well between the random encouragement and the time spent, but it’s not as significant as the former correlation. This can also be validated, as the random encouragement should not have a significant impact on the time spent by the users on the application."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#ploting-regression-lines-by-running-the-analysis-with-half-the-bandwidth",
    "href": "content/01_journal/10_rdd.html#ploting-regression-lines-by-running-the-analysis-with-half-the-bandwidth",
    "title": "Regression Discontinuity",
    "section": "Ploting regression lines by running the analysis with half the bandwidth",
    "text": "Ploting regression lines by running the analysis with half the bandwidth\n\n# Define cut-off\nc0 &lt;- 60\n# Specify bandwidth\nbw &lt;- (c0 + c(-2.5, 2.5))\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df_coupon %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df_coupon %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n#&gt; [1] 181   4\n\n\n\nLATE\n\n\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE: %.2f\", late)\n\n#&gt; [1] \"LATE: 7.36\"\n\n\n\nPloting\n\n\nlibrary(ggplot2)\n\n# Minimum and maximum for y-axis limits\nmin_y &lt;- min(df_bw$purchase_after)\nmax_y &lt;- max(df_bw$purchase_after)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw &lt;- \n  ggplot(df_bw, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, linewidth = 2) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, linewidth = 2) +\n  geom_segment(aes(x = c0, xend = bw[2], y = y0, yend = y0),\n             linetype = \"dotted\") +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dotted\") +\n  annotate(\"text\", x = c0+2, y = mean(c(y1, y0)-2),\n           label = sprintf(\"Difference: %.2f\", (y1 - y0)),\n           fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#ploting-regression-lines-by-running-the-analysis-with-double-the-bandwidth",
    "href": "content/01_journal/10_rdd.html#ploting-regression-lines-by-running-the-analysis-with-double-the-bandwidth",
    "title": "Regression Discontinuity",
    "section": "Ploting regression lines by running the analysis with double the bandwidth",
    "text": "Ploting regression lines by running the analysis with double the bandwidth\n\n# Specify bandwidth\nbw &lt;- (c0 + c(-10, 10))\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below &lt;- df_coupon %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\ndf_bw_above &lt;- df_coupon %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n\ndf_bw &lt;- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n\n#&gt; [1] 629   4\n\n\n\nLATE\n\n\nmodel_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\ny1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate &lt;- y1 - y0\nsprintf(\"LATE: %.2f\", late)\n\n#&gt; [1] \"LATE: 9.51\"\n\n\n\nPloting\n\n\n# Minimum and maximum for y-axis limits\nmin_y &lt;- min(df_bw$purchase_after)\nmax_y &lt;- max(df_bw$purchase_after)\n\n# Add lines for vertical distance and change limits of x-axis.\ndep_var_bw &lt;- \n  ggplot(df_bw, aes(x = days_since_last, y = purchase_after, color = coupon)) +\n  geom_vline(xintercept = c0, linewidth = 2) +\n  geom_point(alpha = 0.4, size = 1) +\n  geom_smooth(data = df_bw_below, method = \"lm\", se = F, linewidth = 2) +\n  geom_smooth(data = df_bw_above, method = \"lm\", se = F, linewidth = 2) +\n  geom_segment(aes(x = c0, xend = bw[2], y = y0, yend = y0),\n             linetype = \"dotted\") +\n  geom_segment(aes(x = bw[1], xend = c0, y = y1, yend = y1),\n               linetype = \"dotted\") +\n  annotate(\"text\", x = c0+2, y = mean(c(y1, y0)-2),\n           label = sprintf(\"Difference: %.2f\", (y1 - y0)),\n           fontface = 2) +\n  scale_y_continuous(limits = c(min_y, max_y)) + \n  scale_color_discrete(labels = c(\"No coupon\", \"Coupon\")) +\n  xlab(\"Days since last purchase\") +\n  ylab(\"Purchase after coupon assignment\") +\n  theme(legend.title = element_blank())\ndep_var_bw\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf bandwidth = 2.5, then LATE = 7.36 If bandwidth = 5, then LATE = 7.99 If bandwidth = 10, then LATE = 9.51\n\n\n\nDoubling the bandwidth results in a higher LATE, while halving it leads to a slightly lower LATE. A broader bandwidth may capture a wider range of data, potentially introducing more variation. On the other hand, a smaller bandwidth may fail to capture sufficient treatment effects, resulting in a smaller estimate. The observed sensitivity to the chosen bandwidth suggests an influence on LATE values."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#task-1",
    "href": "content/01_journal/10_rdd.html#task-1",
    "title": "Regression Discontinuity",
    "section": "# Task 1:",
    "text": "# Task 1:"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#task-2",
    "href": "content/01_journal/10_rdd.html#task-2",
    "title": "Regression Discontinuity",
    "section": "# Task 2:",
    "text": "# Task 2:\n\nLoading the data frame\n\n\ndf_shipping  &lt;- readRDS(\"Causal_Data_Science_Data/shipping.rds\")\ndf_shipping\n\n\n\n  \n\n\n\n\nPlot\n\n\ndf_shipping &lt;- df_shipping %&gt;%\n  mutate(shipping_type = ifelse(purchase_amount &gt; 30, \"Zero-cost Shipping\", \"cost Shipping\"))\n\nggplot(df_shipping, aes(x = purchase_amount, y = shipping_type, color = shipping_type)) +\n  geom_vline(xintercept = 30, color = palette()[2], linetype = \"dashed\") +\n  geom_point(alpha = 0.2, position = position_jitter()) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"Zero-cost Shipping\", \"cost Shipping\"))+\n  scale_color_discrete(labels = c(\"Zero-cost Shipping\", \"cost Shipping\")) +\n  xlab(\"Purchase Amount\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThe argument is backed by the easy-to-see 30€ cut-off line and how purchases are spread on both sides. This matches the info given for Zero-cost Shipping on purchases over 30€. The dotted line at 30€ makes it clear where the cut-off is. The chart shows well how purchase_amount helps tell which purchases get Zero-cost Shipping and which ones have shipping costs."
  },
  {
    "objectID": "content/01_journal/10_rdd.html#task---1",
    "href": "content/01_journal/10_rdd.html#task---1",
    "title": "Regression Discontinuity",
    "section": "# Task - 1",
    "text": "# Task - 1"
  },
  {
    "objectID": "content/01_journal/10_rdd.html#task---2",
    "href": "content/01_journal/10_rdd.html#task---2",
    "title": "Regression Discontinuity",
    "section": "# Task - 2",
    "text": "# Task - 2\n\nLoading the data frame\n\n\ndf_shipping  &lt;- readRDS(\"Causal_Data_Science_Data/shipping.rds\")\ndf_shipping\n\n\n\n  \n\n\n\n\nPlot\n\n\ndf_shipping &lt;- df_shipping %&gt;%\n  mutate(shipping_type = ifelse(purchase_amount &gt; 30, \"Zero-cost Shipping\", \"cost Shipping\"))\n\nggplot(df_shipping, aes(x = purchase_amount, y = shipping_type, color = shipping_type)) +\n  geom_vline(xintercept = 30, color = palette()[2], linetype = \"dashed\") +\n  geom_point(alpha = 0.2, position = position_jitter()) +\n  guides(scale = \"none\") +\n  scale_y_discrete(labels = c(\"Zero-cost Shipping\", \"cost Shipping\"))+\n  scale_color_discrete(labels = c(\"Zero-cost Shipping\", \"cost Shipping\")) +\n  xlab(\"Purchase Amount\") +\n  ylab(\"\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThe argument is backed by the easy-to-see 30€ cut-off line and how purchases are spread on both sides. This matches the info given for Zero-cost Shipping on purchases over 30€. The dotted line at 30€ makes it clear where the cut-off is. The chart shows well how purchase_amount helps tell which purchases get Zero-cost Shipping and which ones have shipping costs."
  },
  {
    "objectID": "content/01_journal/08_did.html#month-hospital",
    "href": "content/01_journal/08_did.html#month-hospital",
    "title": "Difference-in-Differences",
    "section": "month + hospital",
    "text": "month + hospital\n\nmodel1&lt;- lm(satis ~ month + hospital, data = df_hospdd)\nmodel1\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ month + hospital, data = df_hospdd)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)        month     hospital  \n#&gt;     3.75972      0.07207     -0.01760"
  },
  {
    "objectID": "content/01_journal/08_did.html#as.factormonth-as.factorhospital",
    "href": "content/01_journal/08_did.html#as.factormonth-as.factorhospital",
    "title": "Difference-in-Differences",
    "section": "as.factor(month) + as.factor(hospital)",
    "text": "as.factor(month) + as.factor(hospital)\n\nmodel2 &lt;- lm(satis ~ as.factor(month) + as.factor(hospital), data = df_hospdd)\nmodel2\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ as.factor(month) + as.factor(hospital), \n#&gt;     data = df_hospdd)\n#&gt; \n#&gt; Coefficients:\n#&gt;           (Intercept)      as.factor(month)2      as.factor(month)3  \n#&gt;              3.419332              -0.009608               0.021969  \n#&gt;     as.factor(month)4      as.factor(month)5      as.factor(month)6  \n#&gt;              0.349354               0.343235               0.348800  \n#&gt;     as.factor(month)7   as.factor(hospital)2   as.factor(hospital)3  \n#&gt;              0.341444               0.408566               0.533625  \n#&gt;  as.factor(hospital)4   as.factor(hospital)5   as.factor(hospital)6  \n#&gt;              0.227510              -0.145353               0.447863  \n#&gt;  as.factor(hospital)7   as.factor(hospital)8   as.factor(hospital)9  \n#&gt;              1.404416               0.071876              -1.518515  \n#&gt; as.factor(hospital)10  as.factor(hospital)11  as.factor(hospital)12  \n#&gt;              1.682845               0.220965              -0.095303  \n#&gt; as.factor(hospital)13  as.factor(hospital)14  as.factor(hospital)15  \n#&gt;              0.495593               0.233043              -0.144494  \n#&gt; as.factor(hospital)16  as.factor(hospital)17  as.factor(hospital)18  \n#&gt;              1.414268               0.423543               0.153276  \n#&gt; as.factor(hospital)19  as.factor(hospital)20  as.factor(hospital)21  \n#&gt;             -1.169296              -0.376607               0.770343  \n#&gt; as.factor(hospital)22  as.factor(hospital)23  as.factor(hospital)24  \n#&gt;              0.375321               0.277726              -0.732120  \n#&gt; as.factor(hospital)25  as.factor(hospital)26  as.factor(hospital)27  \n#&gt;              0.222480              -0.209747              -0.822648  \n#&gt; as.factor(hospital)28  as.factor(hospital)29  as.factor(hospital)30  \n#&gt;              0.288001              -0.175443              -0.591916  \n#&gt; as.factor(hospital)31  as.factor(hospital)32  as.factor(hospital)33  \n#&gt;              0.088091              -0.747340              -0.877969  \n#&gt; as.factor(hospital)34  as.factor(hospital)35  as.factor(hospital)36  \n#&gt;             -0.424406              -0.069883               1.714149  \n#&gt; as.factor(hospital)37  as.factor(hospital)38  as.factor(hospital)39  \n#&gt;             -0.283590              -0.510800              -0.447491  \n#&gt; as.factor(hospital)40  as.factor(hospital)41  as.factor(hospital)42  \n#&gt;              0.697539              -0.573729               0.457143  \n#&gt; as.factor(hospital)43  as.factor(hospital)44  as.factor(hospital)45  \n#&gt;             -1.196426              -0.389582              -0.637743  \n#&gt; as.factor(hospital)46  \n#&gt;             -0.345502\n\n\n\nDifference:\n\n\nmonth + hospital treats them as continuous variables, while as.factor(month) + as.factor(hospital) treats them as categorical (factor) variables. The latter is more appropriate when dealing with factors like months and hospitals, where the order and magnitude might not have meaningful interpretations.\nFor fixed effects representing categorical variables like months and hospitals, it is generally recommended to use as.factor() to ensure that the model considers them as discrete categories rather than continuous variables. This approach is more suitable for capturing the fixed effects accurately in the context of the analysis."
  },
  {
    "objectID": "content/01_journal/08_did.html#mean-satisfaction-for-treated-and-control-hospitals-before-and-after-the-treatment",
    "href": "content/01_journal/08_did.html#mean-satisfaction-for-treated-and-control-hospitals-before-and-after-the-treatment",
    "title": "Difference-in-Differences",
    "section": "Mean satisfaction for treated and control hospitals before and after the treatment",
    "text": "Mean satisfaction for treated and control hospitals before and after the treatment\n\n# Step 1: Subset the data for treatment and control hospitals\ntreated_hospitals &lt;- df_hospdd %&gt;% filter(procedure == 1)\ncontrol_hospitals &lt;- df_hospdd %&gt;% filter(procedure == 0)\n\n# Step 2: Compute mean satisfaction for treated hospitals before and after treatment\nmean_satis_before_treated &lt;- treated_hospitals %&gt;%\n  filter(month &lt; 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\nmean_satis_before_treated = 0\n\nmean_satis_after_treated &lt;- treated_hospitals %&gt;%\n  filter(month &gt;= 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\n\n# Step 3: Compute mean satisfaction for control hospitals before and after treatment\nmean_satis_before_control &lt;- control_hospitals %&gt;%\n  filter(month &lt; 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\n\nmean_satis_after_control &lt;- control_hospitals %&gt;%\n  filter(month &gt;= 4) %&gt;%\n  pull(satis) %&gt;%\n  mean(na.rm = TRUE)\n\n# Step 4: Print the results\ncat(\n  \"Mean Satisfaction -\", \"\\n\",\n  \"Before (Treated):\", mean_satis_before_treated, \"\\n\",\n  \"After (Treated):\", mean_satis_after_treated, \"\\n\",\n  \"Before (Control):\", mean_satis_before_control, \"\\n\",\n  \"After (Control):\", mean_satis_after_control, \"\\n\"\n)\n\n#&gt; Mean Satisfaction - \n#&gt;  Before (Treated): 0 \n#&gt;  After (Treated): 4.363351 \n#&gt;  Before (Control): 3.447765 \n#&gt;  After (Control): 3.38249"
  },
  {
    "objectID": "content/01_journal/08_did.html#did",
    "href": "content/01_journal/08_did.html#did",
    "title": "Difference-in-Differences",
    "section": "DiD",
    "text": "DiD\n\ndiff_before_treatment = mean_satis_before_treated - mean_satis_before_control\ndiff_after_treatment = mean_satis_after_treated - mean_satis_after_control\n\ndid_estimate &lt;- diff_after_treatment - diff_before_treatment\ndid_estimate\n\n#&gt; [1] 4.428627"
  }
]